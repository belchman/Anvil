# Anvil: The Pitch

---

## FOR: Why This Is the Correct Approach (2 minutes)

The core insight of Anvil is that LLMs are unreliable authors but excellent followers of process. Left unconstrained, an LLM will hallucinate requirements, skip edge cases, write untested code, and ship it with a cheerful summary. Anvil treats this failure mode as a first-class engineering problem and solves it with structure.

**The pipeline forces the agent to think before it types.** Interrogation before documentation. Documentation before specification. Specification before implementation. Implementation only to satisfy failing tests. This is not ceremony for ceremony's sake -- it is the only known way to prevent an LLM from confabulating a feature that nobody asked for. The BDD discipline (red/green/refactor) constrains the agent to write exactly the code required and nothing more. Every human software team that has tried "just let the developers code" has learned this lesson; Anvil applies it to agents.

**Cross-model review eliminates the same-brain problem.** Sonnet writes specs, Opus implements. Opus generates holdouts, Sonnet validates. The review gates use dual-pass evaluation with position bias mitigation. This is not theater -- it is the LLM equivalent of code review by a different person. A model reviewing its own output will find fewer errors than a different model reviewing it. Anvil enforces this separation structurally, not by policy.

**The circuit breakers are honest about cost.** A $50 ceiling, per-phase budgets, stagnation detection, a kill switch file -- these exist because the framework's author knows that autonomous agents can burn money in infinite loops. Most agent frameworks pretend this problem does not exist. Anvil puts a dollar sign on every phase and stops the pipeline when the math stops making sense.

**Context discipline solves the real bottleneck.** LLM context windows are not infinite, and treating them as infinite produces degraded output as the window fills. Anvil's fidelity modes, pyramid summaries, and compaction rules ensure each phase starts with exactly the context it needs and nothing more. Fresh sessions between phases mean no accumulated hallucination drift.

**It is portable and self-testing.** Drop it into any project. 185 self-tests verify structural integrity. No hardcoded paths, no project-specific assumptions, no vendor lock-in beyond the Claude CLI. The config is centralized in one file. The routing graph is a DOT file you can visualize. The whole thing is legible.

The framework encodes a hard-won truth: autonomous agents need more process discipline than humans, not less. Anvil provides that discipline.

---

## AGAINST: Why This Is Not the Correct Approach (2 minutes)

Let us be blunt. Anvil is a 1,000-line bash script and a 1,000-line Python script that orchestrate an LLM to talk to itself approximately 15-20 times before producing a pull request. The fundamental question is: does wrapping an LLM in bureaucracy make it smarter, or does it just make it slower and more expensive?

**The "cross-model review" is security theater.** Sonnet and Opus are trained on the same data, by the same company, with the same RLHF objectives. Calling this "cross-model diversity" is like calling two employees from the same department "independent reviewers." The dual-pass position bias mitigation sounds rigorous until you realize both passes are asking the same model family to evaluate the same text. You are paying double for a correlation of 0.95. The holdout scenarios are generated by an LLM and validated by an LLM. At no point does a human or a real test suite verify that the holdouts are meaningful. You are testing hallucinations against hallucinations.

**The cost is absurd.** A "full" pipeline run costs $40-50 and takes 30+ minutes to produce a single PR. A competent developer with Copilot can do the same work in the same time for $0.10 of API calls. The "quick" tier at $8-15 skips reviews, holdouts, and security -- which are the only things that justify the framework's existence. The framework's value proposition is rigorous quality gates, but its cost optimization strategy is to skip them.

**The process is cargo cult BDD.** Real BDD requires a human product owner who defines behavior in business language. Anvil has an LLM interrogating itself, generating its own requirements, writing its own acceptance criteria, then implementing against them. This is not BDD -- it is an LLM writing fiction in Gherkin syntax. The "interrogation" phase searches MCP sources that may or may not exist, and when they do not exist, it makes "tagged assumptions." A tagged hallucination is still a hallucination. The [ASSUMPTION: rationale] tag does not make the assumption correct; it just makes it documented.

**Context discipline is solving a self-inflicted problem.** The framework needs fidelity modes, compaction rules, and pyramid summaries because it generates enormous volumes of intermediate text that serve no purpose except to feed into the next phase. A simpler system that did not produce 11 documentation templates before writing code would not need a context budget. The documentation is not for humans -- humans never read `OBSERVABILITY.md` or `ROLLOUT_PLAN.md` for a logout button. The documentation exists to give the next LLM phase something to read. You are paying for an LLM to write documents that only another LLM will read.

**The religious framing masks fragility.** Calling your process "The Way" and your config file "The Sacrament of Specification" does not make it robust. It makes it hard to question. The framework has zero production deployments. The test suite validates file existence and bash syntax, not pipeline outcomes. Nobody has measured whether Anvil PRs are better than a single `claude -p "implement this ticket"` call. The 185 self-tests check that the framework's files exist and parse correctly. They do not check that the framework produces correct software.

**The portability claim is aspirational.** The framework requires Claude Code CLI, jq, bc, git, gh, and Node.js 18+. The Python runner requires `claude-agent-sdk-python`, which is a proprietary SDK coupled to a single vendor. "Drop it into any project" means "drop it into any project that uses Claude, has all these dependencies, and is willing to spend $50 per PR." That is not portability. That is a very specific niche.

The honest summary: Anvil is an impressive engineering effort that solves the problem of "how do I make an LLM follow a process" without first proving that the process produces better outcomes than no process at all.

---

## Score

| Dimension | FOR | AGAINST |
|-----------|-----|---------|
| Core thesis (process > freestyle) | Strong. Evidence from human software engineering supports this. | Medium. The analogy to human teams may not transfer to LLMs, which fail differently. |
| Cross-model review | Medium. Separation of generator and reviewer is sound in principle. | Strong. Same model family, same training data. Empirical benefit unproven. |
| Cost management | Strong. Circuit breakers and budgets are genuinely useful. | Strong. The baseline cost is too high for the value delivered. |
| Context discipline | Strong. Real problem, well-engineered solution. | Medium. The problem is partially self-inflicted by the framework's own verbosity. |
| BDD enforcement | Medium. Constraining LLM output is valuable. | Strong. LLM-generated specs lack the human intent that makes BDD meaningful. |
| Production readiness | Weak. Zero deployments, no outcome measurement. | Strong. This is a prototype, not a proven system. |
| Portability | Medium. Centralized config, no hardcoded paths. | Medium. Heavy dependency chain, single-vendor lock-in. |

**Overall: 5.5 / 10**

The architecture is thoughtful and the engineering is solid. The framework correctly identifies that autonomous agents need guardrails, cost controls, and structured process. But it has not proven that its specific guardrails produce better outcomes than simpler alternatives. The cross-model review is its most novel claim and its weakest empirical foundation. The cost structure makes it impractical for the small-to-medium changes that constitute 80% of real development work.

Anvil is a well-built answer to a question that has not yet been validated: "Does wrapping an LLM in an 11-phase pipeline with LLM-as-judge review gates produce measurably better software than letting the same LLM work with lighter constraints?" Until that question has an empirical answer, Anvil is an expensive hypothesis.

The right next step is not more framework engineering. It is a controlled comparison: 20 tickets through Anvil, 20 tickets through a simple single-prompt approach, measured by defect rate, human review time, and total cost. If Anvil wins that comparison, the score goes to 8. If it does not, the score goes to 3. Right now, we do not know, and that uncertainty is the framework's biggest risk.
