| Dimension | Weight | FOR | AGAINST | Justification |
|-----------|--------|-----|---------|---------------|
| **Correctness** | 25% | 7 | 4 | **FOR**: Claims are specific, verifiable, and unusually honest — admits same score at 13x cost on the flagship BENCH-6 comparison, includes "when to skip Anvil" guidance. **AGAINST**: All benchmarks are self-authored measuring a self-authored tool; N=5 variance studies are too small to draw the statistical conclusions implied; extrapolating "2-3 tickets per project where silent defects slip through" from a 10-ticket suite is a stretch. |
| **Cost efficiency** | 20% | 6 | 6 | **FOR**: Tiered approach is genuinely smart — guard/nano tiers add minimal overhead, cost ceilings and per-phase budgets prevent runaway spend. **AGAINST**: The flagship head-to-head shows $0.44 vs $5.67 (13x) for identical quality (90/100 both); freestyle already achieves 96-100% on most tasks at $2.30 total — the marginal value of governance is hard to justify economically when quality is already high. |
| **Rigor** | 15% | 8 | 4 | **FOR**: Genuinely rigorous: spec-first BDD, non-LLM external validators by default, cross-model diversity, discriminating scorer (unfixed baselines at 48/100 and 39/100 prove checks aren't rubber stamps), multiple safety controls with escalation. **AGAINST**: "131 self-tests" are structural checks (file inventory, syntax, cross-refs) not functional tests of quality improvement; no measurement of gate false-positive/negative rates. |
| **Usability** | 15% | 7 | 3 | **FOR**: Clear quick-start, honest tier guidance ("start with lite"), MCP server integration for zero-friction adoption, "most users change nothing" config philosophy. **AGAINST**: Requires Rust toolchain with no pre-built binaries; deploying ~30 core files into a project is heavy; skills system (/phase0, /interrogate, etc.) has a learning curve the README doesn't fully address. |
| **Evidence** | 10% | 6 | 6 | **FOR**: Concrete per-ticket breakdowns with costs and timing, variance studies, unfixed baselines proving scorer discrimination, dogfooding found 5 real pipeline bugs. **AGAINST**: Entirely self-generated evidence — no external users, no third-party validation, no production deployment data; the key BENCH-6 comparison undermines the quality claim since both approaches score identically. |
| **Innovation** | 10% | 7 | 3 | **FOR**: Tiered governance for AI coding is novel; watchdog escalation (nudge → kill → restart with augmentation), holdout adversarial testing before implementation, and stagnation rerouting are genuinely creative mechanisms. **AGAINST**: The core concept — CI/CD gates applied to AI output — is a natural extension of existing practices; innovation is in assembly and packaging rather than fundamental technique. |
| **Maintenance** | 5% | 7 | 4 | **FOR**: Single 4.2MB Rust binary with clean 9-module separation; legacy runners removed; self-test suite and CI pipeline. **AGAINST**: Rust raises contribution barrier vs. Python/JS; 30 core + 55 benchmark files is significant surface area; hard dependency on Claude Code CLI means upstream breaking changes propagate. |

---

**TOTAL FOR: 6.85**
**TOTAL AGAINST: 4.35**

---

**Net assessment**: The README is honest to a fault — which is both its greatest strength and its commercial weakness. The BENCH-6 head-to-head transparently showing identical scores at 13x cost will make skeptics stop reading. The real value proposition (governance, auditability, blocking rather than silently shipping defects) is buried under benchmark data that, on first scan, argues against adoption. The technical rigor is genuine but the evidence base is too self-referential to be convincing to an external audience. A stronger pitch would lead with the "blocked vs. silently shipped" distinction and de-emphasize score parity.
